{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4771786,"sourceType":"datasetVersion","datasetId":2761867}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda create -n py38 python=3.8 -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda env list","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:30:03.148339Z","iopub.execute_input":"2024-08-19T17:30:03.148723Z","iopub.status.idle":"2024-08-19T17:30:05.050594Z","shell.execute_reply.started":"2024-08-19T17:30:03.148694Z","shell.execute_reply":"2024-08-19T17:30:05.049242Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"# conda environments:\n#\nbase                     /opt/conda\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/python3 -V","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:17:49.809360Z","iopub.execute_input":"2024-08-13T00:17:49.809814Z","iopub.status.idle":"2024-08-13T00:17:50.864400Z","shell.execute_reply.started":"2024-08-13T00:17:49.809780Z","shell.execute_reply":"2024-08-13T00:17:50.863410Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Python 3.8.19\n","output_type":"stream"}]},{"cell_type":"code","source":"#!git clone https://github.com/duyndh98/PPFR.git\n#!rm -rf TFace\n#!git clone https://github.com/Tencent/TFace.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install dareblopy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install torchjpeg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install pyyaml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install opencv-python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/py38/bin/pip install protobuf==3.20.*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nDATASET_PATH = '/kaggle/input/vietnamese-celebrity-faces'\n\nINDEX_ROOT = '/kaggle/working/index_files'\nif not os.path.exists(INDEX_ROOT):\n    os.mkdir(INDEX_ROOT)\n    \nindex_path = os.path.join(INDEX_ROOT, 'vietnamese-celebrity-faces.index.txt')\nif os.path.exists(index_path):\n    os.remove(index_path)\n\nceleb_id = 0\nwith open(index_path, 'w') as out_file:\n    for job_name in os.listdir(DATASET_PATH):\n        job_path = os.path.join(DATASET_PATH, job_name)\n        for celeb_name in os.listdir(job_path):\n            celeb_path = os.path.join(job_path, celeb_name)\n            for img_name in os.listdir(celeb_path):\n                out_file.write(\"{}\\t{}\\n\".format(os.path.join(job_name, celeb_name, img_name), celeb_id))\n            celeb_id += 1","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:15:16.864335Z","iopub.execute_input":"2024-08-12T19:15:16.865258Z","iopub.status.idle":"2024-08-12T19:15:20.969772Z","shell.execute_reply.started":"2024-08-12T19:15:16.865222Z","shell.execute_reply":"2024-08-12T19:15:20.968985Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!cp -r '/kaggle/working/TFace/recognition' '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:16:54.494296Z","iopub.execute_input":"2024-08-12T19:16:54.494985Z","iopub.status.idle":"2024-08-12T19:16:55.537273Z","shell.execute_reply.started":"2024-08-12T19:16:54.494947Z","shell.execute_reply":"2024-08-12T19:16:55.536175Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/recognition/tasks/minusface')\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:28:50.594742Z","iopub.execute_input":"2024-08-13T00:28:50.595566Z","iopub.status.idle":"2024-08-13T00:28:50.603015Z","shell.execute_reply.started":"2024-08-13T00:28:50.595534Z","shell.execute_reply":"2024-08-13T00:28:50.601860Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/recognition/tasks/minusface'"},"metadata":{}}]},{"cell_type":"code","source":"%%writefile '/kaggle/working/recognition/torchkit/hooks/learning_rate_hook.py'\nimport logging\nfrom bisect import bisect\nfrom .base_hook import Hook\n\n\ndef set_optimizer_lr(optimizer, lr):\n    if isinstance(optimizer, dict):\n        backbone_opt, head_opts = optimizer['backbone'], optimizer['heads']\n        if len(backbone_opt.param_groups) > 1: # is stage 1\n            rate = 0.1\n            backbone_opt.param_groups[0]['lr'] = lr * rate\n            backbone_opt.param_groups[1]['lr'] = lr\n            for _, head_opt in head_opts.items():\n                for param_group in head_opt.param_groups:\n                    param_group['lr'] = lr\n        else: # is stage 2\n            for param_group in backbone_opt.param_groups:\n                param_group['lr'] = lr\n    else:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef warm_up_lr(step, warmup_step, init_lr, optimizer):\n    \"\"\" Warm up learning rate when batch step below warmup steps\n    \"\"\"\n\n    lr = step * init_lr / warmup_step\n    if step % 500 == 0:\n        logging.info(\"Current step {}, learning rate {}\".format(step, lr))\n\n    set_optimizer_lr(optimizer, lr)\n\n\ndef adjust_lr(epoch, learning_rates, stages, optimizer):\n    \"\"\" Decay the learning rate based on schedule\n    \"\"\"\n\n    pos = bisect(stages, epoch)\n    lr = learning_rates[pos]\n    logging.info(\"Current epoch {}, learning rate {}\".format(epoch + 1, lr))\n\n    set_optimizer_lr(optimizer, lr)\n\n\nclass LearningRateHook(Hook):\n    \"\"\" LearningRate Hook, adjust learning rate in training\n    \"\"\"\n    def __init__(self,\n                 learning_rates,\n                 stages,\n                 warmup_step):\n        \"\"\" Create a ``LearningRateHook`` object\n\n            Args:\n            learning_rates: all learning rates value\n            stages: learning rate adjust stages value\n            warmup_step: step num of warmup\n        \"\"\"\n\n        self.learning_rates = learning_rates\n        self.stages = stages\n        if len(self.learning_rates) != len(self.stages) + 1:\n            raise RuntimeError(\"Learning_rates size should be one larger than stages size\")\n        self.init_lr = self.learning_rates[0]\n        self.warmup_step = warmup_step\n\n    def before_train_iter(self, task, step, epoch):\n        global_step = epoch * task.step_per_epoch + step\n        if self.warmup_step > 0 and global_step <= self.warmup_step:\n            warm_up_lr(global_step, self.warmup_step, self.init_lr, task.opt)\n\n    def before_train_epoch(self, task, epoch):\n        adjust_lr(epoch, self.learning_rates, self.stages, task.opt)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:28:58.659843Z","iopub.execute_input":"2024-08-13T00:28:58.660185Z","iopub.status.idle":"2024-08-13T00:28:58.667936Z","shell.execute_reply.started":"2024-08-13T00:28:58.660159Z","shell.execute_reply":"2024-08-13T00:28:58.666697Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/recognition/torchkit/hooks/learning_rate_hook.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile '/kaggle/working/recognition/tasks/minusface/train.yaml'\nSEED: 1337 # random seed for reproduce results\nDATA_ROOT: '/kaggle/input/vietnamese-celebrity-faces' # [fill in this blank] the parent directory where your train/val/test data are stored\nINDEX_ROOT: '/kaggle/working/index_files' # [fill in this blank] the parent directory for index\nDATASETS:\n  - name: 'vietnamese-celebrity-faces.index'# [fill in this blank] the name of your dataset\n    batch_size: 16\n    weight: 1.0\n    scale: 64\n    margin: 0.5\n\nBACKBONE_RESUME: \"\"\nHEAD_RESUME: \"\"\nMETA_RESUME: \"\"\n\nINPUT_SIZE: [ 112, 112 ]\nBACKBONE_NAME: 'IR_18' # support: ['IR_18', 'IR_50']\nEMBEDDING_SIZE: 512\n\nMODEL_ROOT: '/kaggle/working/ckpt_s1' # the root to buffer your checkpoints\nLOG_ROOT: '/kaggle/working/tensorboard_s1' # the root to log your train/val status\n\nDIST_FC: true\nHEAD_NAME: \"ArcFace\" # support:  ['ArcFace', 'CurricularFace', 'CosFace']\nLOSS_NAME: 'DistCrossEntropy' # support: ['DistCrossEntropy', 'Softmax']\n\nRGB_MEAN: [ 0.5, 0.5, 0.5 ] # for normalize inputs to [-1, 1]\nRGB_STD: [ 0.5, 0.5, 0.5 ]\n\nLRS: [ 0.01, 0.001, 0.0001, 0.00001 ]\nWARMUP_STEP: -1\nSTAGES: [ 10, 18, 22 ]\n\nSTART_EPOCH: 0 # start epoch\nNUM_EPOCH: 18 # total epoch number\nSAVE_EPOCHS: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18 ]\n\nWEIGHT_DECAY: 0.0005 # do not apply to batch_norm parameters\nMOMENTUM: 0.9\n\nWORLD_SIZE: 1\nRANK: 0\nLOCAL_RANK: 0\nDIST_BACKEND: 'nccl'\nDIST_URL: 'env://'\n\nNUM_WORKERS: 8\n\nAMP: false # fp16 for backbone\n\n# MinusFace\nMETHOD: MinusFace\nTASK: stage1 # toy, stage1, stage2\nNUM_DUPS: 1\nNUM_AUG: 3 # multiplier for data augmentation\nTASK_BACKBONE: 'IR_18' # IR_18, IR_50\nPRETRAIN_CKPT: '' # [fill in this blank] to train the recognition model requires pretrained MinusFace checkpoint\nTASK_VER: 3","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:40:51.988622Z","iopub.execute_input":"2024-08-13T00:40:51.989004Z","iopub.status.idle":"2024-08-13T00:40:51.995932Z","shell.execute_reply.started":"2024-08-13T00:40:51.988974Z","shell.execute_reply":"2024-08-13T00:40:51.994952Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/recognition/tasks/minusface/train.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf '/kaggle/working/ckpt_s1'\n!mkdir '/kaggle/working/ckpt_s1'\n!rm -rf '/kaggle/working/tensorboard_s1'\n!mkdir '/kaggle/working/tensorboard_s1'","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:41:26.970289Z","iopub.execute_input":"2024-08-13T00:41:26.970688Z","iopub.status.idle":"2024-08-13T00:41:31.136308Z","shell.execute_reply.started":"2024-08-13T00:41:26.970660Z","shell.execute_reply":"2024-08-13T00:41:31.135006Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"%%writefile '/kaggle/working/recognition/tasks/partialface/base_task.py'\nimport os\nimport sys\nimport copy\nimport logging\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\nfrom torchkit.task.base_task import BaseTask\nfrom torchkit.backbone import get_model\nfrom torchkit.head import get_head\nfrom torchkit.util import get_class_split, separate_resnet_bn_paras\nfrom torchkit.data import MultiDataset, MultiDistributedSampler\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s: %(message)s')\n\n\nclass AugmentedMultiDataset(MultiDataset):\n    def __init__(self, data_root, index_root, names, transform, num_aug, **kwargs) -> None:\n        super().__init__(data_root, index_root, names, transform, **kwargs)\n        self.num_aug = num_aug\n\n    def _build_inputs(self, world_size=None, rank=None):\n        \"\"\" Read index file and saved in ``self.inputs``\n            If ``self.is_shard`` is True, ``total_sample_nums`` > ``sample_nums``\n        \"\"\"\n\n        for i, name in enumerate(self.names):\n            index_file = os.path.join(self.index_root, name + \".txt\")\n            self.index_parser.reset()\n            self.inputs[name] = []\n            with open(index_file, 'r') as f:\n                for line_i, line in enumerate(f):\n                    sample = self.index_parser(line)\n                    if self.is_shard is False:\n                        # ============== PartialFace / MinusFace ==============\n                        for i in range(self.num_aug):\n                            self.inputs[name].append(sample)\n                        # =====================================================\n                    else:\n                        if line_i % world_size == rank:\n                            self.inputs[name].append(sample)\n                        else:\n                            pass\n            self.class_nums[name] = self.index_parser.class_num + 1\n            self.sample_nums[name] = len(self.inputs[name])\n            if self.is_shard:\n                self.total_sample_nums[name] = self.index_parser.sample_num\n                logging.info(\"Dataset %s, class_num %d, total_sample_num %d, sample_num %d\" % (\n                    name, self.class_nums[name], self.total_sample_nums[name], self.sample_nums[name]))\n            else:\n                logging.info(\"Dataset %s, class_num %d, sample_num %d\" % (\n                    name, self.class_nums[name], self.sample_nums[name]))\n\n\nclass LocalBaseTask(BaseTask):\n    def __init__(self, cfg_file):\n        super().__init__(cfg_file=cfg_file)\n        # if self.cfg['METHOD'] == 'PartialFace':\n        #     self.num_aug = self.cfg['NUM_AUG']\n        #     self.num_chs = self.cfg['NUM_CHS']\n\n    def make_inputs(self):\n        \"\"\" make datasets\n        \"\"\"\n        rgb_mean = self.cfg['RGB_MEAN']\n        rgb_std = self.cfg['RGB_STD']\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((112, 112)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=rgb_mean, std=rgb_std)\n        ])\n\n        ds_names = list(self.branches.keys())\n        # ============== PartialFace / MinusFace ==============\n        ds = AugmentedMultiDataset(self.cfg['DATA_ROOT'], self.cfg['INDEX_ROOT'], ds_names,\n                                   transform, self.cfg['NUM_AUG'])\n        # =====================================================\n\n        ds.make_dataset(shard=False)\n        self.class_nums = ds.class_nums\n\n        sampler = MultiDistributedSampler(ds, self.batch_sizes)\n        self.train_loader = DataLoader(ds, sum(self.batch_sizes), shuffle=False,\n                                       num_workers=self.cfg[\"NUM_WORKERS\"], pin_memory=True,\n                                       sampler=sampler, drop_last=False)\n\n        self.step_per_epoch = len(self.train_loader)\n        logging.info(\"Step_per_epoch = %d\" % self.step_per_epoch)\n\n    def make_model(self):\n        \"\"\" build training backbone and heads\n        \"\"\"\n\n        # ============== PartialFace / MinusFace ==============\n        if self.cfg['METHOD'] == 'PartialFace':\n\n            backbone_name = self.cfg['BACKBONE_NAME']\n            backbone_model = get_model(backbone_name)\n            self.backbone = backbone_model(self.input_size)\n\n            self.backbone.input_layer = nn.Sequential(nn.Conv2d(self.cfg['NUM_CHS'] * 3, 64, (3, 3), 1, 1, bias=False),\n                                                      nn.BatchNorm2d(64), nn.PReLU(64))\n\n            logging.info(\"{} Backbone Generated\".format(backbone_name))\n\n        else:  # self.cfg['METHOD'] == 'MinusFace'\n            from tasks.minusface.minusface import MinusBackbone\n\n            generator, recognizer = None, None\n\n            recognizer = get_model(self.cfg['TASK_BACKBONE'])([112, 112])\n            print('Recognizer is {}'.format(self.cfg['TASK_BACKBONE']))\n\n            if self.cfg['TASK'] == 'stage2':\n                pretrain_backbone = MinusBackbone(mode='stage1',\n                                                  recognizer=get_model(self.cfg['TASK_BACKBONE'])([112, 112]))\n                pretrain_backbone.load_state_dict(torch.load(self.cfg['PRETRAIN_CKPT']))\n                pretrain_backbone.generator.mode = self.cfg['TASK']\n                generator = copy.deepcopy(pretrain_backbone.generator)\n                print('Load pretrain ckpt: ', self.cfg['PRETRAIN_CKPT'])\n\n            self.backbone = MinusBackbone(mode=self.cfg['TASK'], n_duplicate=1, generator=generator,\n                                          recognizer=recognizer)\n            logging.info(f\"Minus {self.cfg['TASK']} Backbone Generated\")\n        # =====================================================\n\n        self.backbone.cuda()\n\n        embedding_size = self.cfg['EMBEDDING_SIZE']\n        self.class_shards = []\n        metric = get_head(self.cfg['HEAD_NAME'], dist_fc=self.dist_fc)\n\n        for name, branch in self.branches.items():\n            class_num = self.class_nums[name]\n            class_shard = get_class_split(class_num, self.world_size)\n            self.class_shards.append(class_shard)\n            logging.info('Split FC: {}'.format(class_shard))\n\n            init_value = torch.FloatTensor(embedding_size, class_num)\n            init.normal_(init_value, std=0.01)\n            head = metric(in_features=embedding_size,\n                          gpu_index=self.rank,\n                          weight_init=init_value,\n                          class_split=class_shard,\n                          scale=branch.scale,\n                          margin=branch.margin)\n            del init_value\n            head = head.cuda()\n            self.heads[name] = head\n\n    def get_optimizer(self):\n        \"\"\" build optimizers\n        \"\"\"\n\n        learning_rates = self.cfg['LRS']\n        init_lr = learning_rates[0]\n        weight_decay = self.cfg['WEIGHT_DECAY']\n        momentum = self.cfg['MOMENTUM']\n\n        # ===================== MinusFace =====================\n        if self.cfg['METHOD'] == 'Minusface':\n            if self.cfg['TASK'] == 'stage2':\n                backbone_opt = optim.SGD([{'params': self.backbone.recognizer.parameters(), 'lr': init_lr}],\n                                         weight_decay=weight_decay, lr=init_lr, momentum=momentum)\n            else:\n                backbone_opt = optim.SGD([{'params': self.backbone.generator.parameters(), 'lr': init_lr / 10.},\n                                          {'params': self.backbone.recognizer.parameters(), 'lr': init_lr}],\n                                         weight_decay=weight_decay, lr=init_lr, momentum=momentum)\n        else:\n            backbone_paras_only_bn, backbone_paras_wo_bn = separate_resnet_bn_paras(self.backbone)\n            backbone_opt = optim.SGD([\n                {'params': backbone_paras_wo_bn, 'weight_decay': weight_decay},\n                {'params': backbone_paras_only_bn}], lr=init_lr, momentum=momentum)\n        # =====================================================\n\n        head_opts = OrderedDict()\n        for name, head in self.heads.items():\n            opt = optim.SGD([{'params': head.parameters()}], lr=init_lr, momentum=momentum,\n                            weight_decay=weight_decay)\n            head_opts[name] = opt\n\n        optimizer = {\n            'backbone': backbone_opt,\n            'heads': head_opts,\n        }\n        return optimizer\n\n    def loop_step(self, epoch):\n        \"\"\" Implemented by sub class, which run in every training step\n        \"\"\"\n        raise NotImplementedError()\n\n    def train(self):\n        raise NotImplementedError()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:40:07.692189Z","iopub.execute_input":"2024-08-12T19:40:07.692567Z","iopub.status.idle":"2024-08-12T19:40:07.705008Z","shell.execute_reply.started":"2024-08-12T19:40:07.692537Z","shell.execute_reply":"2024-08-12T19:40:07.704197Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/recognition/tasks/partialface/base_task.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!export CUDA_VISIBLE_DEVICES='0'\n!/opt/conda/envs/py38/bin/python3 -u -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 /kaggle/working/recognition/tasks/minusface/train.py","metadata":{"execution":{"iopub.status.busy":"2024-08-13T00:41:35.439551Z","iopub.execute_input":"2024-08-13T00:41:35.439980Z","iopub.status.idle":"2024-08-13T04:16:47.533311Z","shell.execute_reply.started":"2024-08-13T00:41:35.439945Z","shell.execute_reply":"2024-08-13T04:16:47.532061Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/opt/conda/envs/py38/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\nand will be removed in future. Use torchrun.\nNote that --use-env is set by default in torchrun.\nIf your script expects `--local-rank` argument to be set, please\nchange it to read from `os.environ['LOCAL_RANK']` instead. See \nhttps://pytorch.org/docs/stable/distributed.html#launch-utility for \nfurther instructions\n\n  warnings.warn(\n2024-08-13 00:41:42,060: Dataset vietnamese-celebrity-faces.index, batch_size 16, weight 1.000000, scale 64, margin 0.500000\n2024-08-13 00:41:42,106: Added key: store_based_barrier_key:1 to store for rank: 0\n2024-08-13 00:41:42,106: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n2024-08-13 00:41:42,131: world_size: 1, rank: 0, local_rank: 0\nSEED 1337\nDATA_ROOT /kaggle/input/vietnamese-celebrity-faces\nINDEX_ROOT /kaggle/working/index_files\nDATASETS [{'name': 'vietnamese-celebrity-faces.index', 'batch_size': 16, 'weight': 1.0, 'scale': 64, 'margin': 0.5}]\nBACKBONE_RESUME \nHEAD_RESUME \nMETA_RESUME \nINPUT_SIZE [112, 112]\nBACKBONE_NAME IR_18\nEMBEDDING_SIZE 512\nMODEL_ROOT /kaggle/working/ckpt_s1\nLOG_ROOT /kaggle/working/tensorboard_s1\nDIST_FC True\nHEAD_NAME ArcFace\nLOSS_NAME DistCrossEntropy\nRGB_MEAN [0.5, 0.5, 0.5]\nRGB_STD [0.5, 0.5, 0.5]\nLRS [0.01, 0.001, 0.0001, 1e-05]\nWARMUP_STEP -1\nSTAGES [10, 18, 22]\nSTART_EPOCH 0\nNUM_EPOCH 18\nSAVE_EPOCHS [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\nWEIGHT_DECAY 0.0005\nMOMENTUM 0.9\nWORLD_SIZE 1\nRANK 0\nLOCAL_RANK 0\nDIST_BACKEND nccl\nDIST_URL env://\nNUM_WORKERS 8\nAMP False\nMETHOD MinusFace\nTASK stage1\nNUM_DUPS 1\nNUM_AUG 3\nTASK_BACKBONE IR_18\nPRETRAIN_CKPT \nTASK_VER 3\n2024-08-13 00:41:42,164: Dataset vietnamese-celebrity-faces.index, class_num 224, sample_num 25671\n2024-08-13 00:41:42,164: Dataset vietnamese-celebrity-faces.index, total_size 25680, mine_size 25680, batch_num 1605\n2024-08-13 00:41:42,164: MultiDistributedSampler max_batch_num 1605\n/opt/conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n2024-08-13 00:41:42,165: Step_per_epoch = 1605\nRecognizer is IR_18\n2024-08-13 00:41:42,959: Minus stage1 Backbone Generated\n2024-08-13 00:41:43,916: Split FC: [224]\n2024-08-13 00:41:43,917: FC Start Point: [0, 224]\n2024-08-13 00:41:44,236: Current epoch 1, learning rate 0.01\n[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n2024-08-13 00:41:57,823: Epoch 1 / 18, batch 1 / 1605, 13.5858 sec/batch\n                         loss = [42.090794] gen = [2.312840] fr = [39.777954] spar = [0.000000] prec@1 = [0.000000] prec@5 = [6.250000] \n2024-08-13 00:42:39,939: Epoch 1 / 18, batch 100 / 1605, 0.5570 sec/batch\n                         loss = [42.872643] gen = [2.166298] fr = [40.706345] spar = [0.000000] prec@1 = [0.000000] prec@5 = [6.250000] \n2024-08-13 00:43:24,995: Epoch 1 / 18, batch 200 / 1605, 0.5038 sec/batch\n                         loss = [40.869560] gen = [1.662169] fr = [39.207390] spar = [0.000000] prec@1 = [0.000000] prec@5 = [0.000000] \n2024-08-13 00:44:08,919: Epoch 1 / 18, batch 300 / 1605, 0.4823 sec/batch\n                         loss = [39.492249] gen = [0.960798] fr = [38.531452] spar = [0.000000] prec@1 = [6.250000] prec@5 = [18.750000] \n2024-08-13 00:44:53,470: Epoch 1 / 18, batch 400 / 1605, 0.4731 sec/batch\n                         loss = [38.126476] gen = [0.781004] fr = [37.345474] spar = [0.000000] prec@1 = [25.000000] prec@5 = [31.250000] \n2024-08-13 00:45:37,975: Epoch 1 / 18, batch 500 / 1605, 0.4675 sec/batch\n                         loss = [38.902237] gen = [1.015649] fr = [37.886589] spar = [0.000000] prec@1 = [12.500000] prec@5 = [18.750000] \n2024-08-13 00:46:22,524: Epoch 1 / 18, batch 600 / 1605, 0.4455 sec/batch\n                         loss = [39.786972] gen = [0.753565] fr = [39.033405] spar = [0.000000] prec@1 = [0.000000] prec@5 = [0.000000] \n2024-08-13 00:47:07,106: Epoch 1 / 18, batch 700 / 1605, 0.4457 sec/batch\n                         loss = [37.798950] gen = [0.611318] fr = [37.187634] spar = [0.000000] prec@1 = [0.000000] prec@5 = [18.750000] \n2024-08-13 00:47:51,641: Epoch 1 / 18, batch 800 / 1605, 0.4456 sec/batch\n                         loss = [37.837879] gen = [0.552724] fr = [37.285156] spar = [0.000000] prec@1 = [6.250000] prec@5 = [12.500000] \n2024-08-13 00:48:36,205: Epoch 1 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [38.453632] gen = [0.594713] fr = [37.858921] spar = [0.000000] prec@1 = [12.500000] prec@5 = [25.000000] \n2024-08-13 00:49:20,768: Epoch 1 / 18, batch 1000 / 1605, 0.4456 sec/batch\n                         loss = [37.565029] gen = [0.620208] fr = [36.944820] spar = [0.000000] prec@1 = [12.500000] prec@5 = [25.000000] \n2024-08-13 00:50:05,323: Epoch 1 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [37.985447] gen = [0.610613] fr = [37.374832] spar = [0.000000] prec@1 = [6.250000] prec@5 = [12.500000] \n2024-08-13 00:50:49,888: Epoch 1 / 18, batch 1200 / 1605, 0.4456 sec/batch\n                         loss = [36.827579] gen = [0.430730] fr = [36.396851] spar = [0.000000] prec@1 = [6.250000] prec@5 = [25.000000] \n2024-08-13 00:51:34,447: Epoch 1 / 18, batch 1300 / 1605, 0.4456 sec/batch\n                         loss = [37.126762] gen = [0.585172] fr = [36.541592] spar = [0.000000] prec@1 = [12.500000] prec@5 = [31.250000] \n2024-08-13 00:52:18,998: Epoch 1 / 18, batch 1400 / 1605, 0.4456 sec/batch\n                         loss = [36.460335] gen = [0.395911] fr = [36.064423] spar = [0.000000] prec@1 = [25.000000] prec@5 = [37.500000] \n2024-08-13 00:53:03,576: Epoch 1 / 18, batch 1500 / 1605, 0.4456 sec/batch\n                         loss = [36.353703] gen = [0.442655] fr = [35.911049] spar = [0.000000] prec@1 = [6.250000] prec@5 = [18.750000] \n2024-08-13 00:53:48,209: Epoch 1 / 18, batch 1600 / 1605, 0.4463 sec/batch\n                         loss = [36.544823] gen = [0.480412] fr = [36.064411] spar = [0.000000] prec@1 = [12.500000] prec@5 = [37.500000] \n2024-08-13 00:53:51,425: Save checkpoint at epoch 1 ...\n2024-08-13 00:53:51,425: Current epoch 2, learning rate 0.01\n2024-08-13 00:53:51,972: Epoch 2 / 18, batch 1 / 1605, 0.5461 sec/batch\n                         loss = [35.347668] gen = [0.346227] fr = [35.001442] spar = [0.000000] prec@1 = [25.000000] prec@5 = [37.500000] \n2024-08-13 00:54:36,067: Epoch 2 / 18, batch 100 / 1605, 0.4464 sec/batch\n                         loss = [35.419285] gen = [0.413937] fr = [35.005348] spar = [0.000000] prec@1 = [18.750000] prec@5 = [25.000000] \n2024-08-13 00:55:20,631: Epoch 2 / 18, batch 200 / 1605, 0.4460 sec/batch\n                         loss = [35.536087] gen = [0.444732] fr = [35.091354] spar = [0.000000] prec@1 = [37.500000] prec@5 = [56.250000] \n2024-08-13 00:56:05,188: Epoch 2 / 18, batch 300 / 1605, 0.4459 sec/batch\n                         loss = [36.188011] gen = [0.444370] fr = [35.743641] spar = [0.000000] prec@1 = [25.000000] prec@5 = [31.250000] \n2024-08-13 00:56:49,747: Epoch 2 / 18, batch 400 / 1605, 0.4458 sec/batch\n                         loss = [34.908760] gen = [0.444368] fr = [34.464394] spar = [0.000000] prec@1 = [31.250000] prec@5 = [37.500000] \n2024-08-13 00:57:34,322: Epoch 2 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [34.416546] gen = [0.437618] fr = [33.978928] spar = [0.000000] prec@1 = [25.000000] prec@5 = [50.000000] \n2024-08-13 00:58:18,884: Epoch 2 / 18, batch 600 / 1605, 0.4456 sec/batch\n                         loss = [37.277699] gen = [0.310075] fr = [36.967625] spar = [0.000000] prec@1 = [6.250000] prec@5 = [25.000000] \n2024-08-13 00:59:03,426: Epoch 2 / 18, batch 700 / 1605, 0.4455 sec/batch\n                         loss = [34.265476] gen = [0.483521] fr = [33.781956] spar = [0.000000] prec@1 = [31.250000] prec@5 = [62.500000] \n2024-08-13 00:59:47,983: Epoch 2 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [34.548973] gen = [0.387072] fr = [34.161900] spar = [0.000000] prec@1 = [31.250000] prec@5 = [62.500000] \n2024-08-13 01:00:32,520: Epoch 2 / 18, batch 900 / 1605, 0.4455 sec/batch\n                         loss = [36.187408] gen = [0.344018] fr = [35.843391] spar = [0.000000] prec@1 = [18.750000] prec@5 = [31.250000] \n2024-08-13 01:01:17,068: Epoch 2 / 18, batch 1000 / 1605, 0.4455 sec/batch\n                         loss = [33.219421] gen = [0.400093] fr = [32.819328] spar = [0.000000] prec@1 = [37.500000] prec@5 = [56.250000] \n2024-08-13 01:02:01,635: Epoch 2 / 18, batch 1100 / 1605, 0.4457 sec/batch\n                         loss = [35.638985] gen = [0.448272] fr = [35.190712] spar = [0.000000] prec@1 = [18.750000] prec@5 = [37.500000] \n2024-08-13 01:02:46,194: Epoch 2 / 18, batch 1200 / 1605, 0.4456 sec/batch\n                         loss = [36.314346] gen = [0.343268] fr = [35.971077] spar = [0.000000] prec@1 = [18.750000] prec@5 = [37.500000] \n2024-08-13 01:03:30,767: Epoch 2 / 18, batch 1300 / 1605, 0.4457 sec/batch\n                         loss = [36.687710] gen = [0.355338] fr = [36.332371] spar = [0.000000] prec@1 = [18.750000] prec@5 = [18.750000] \n2024-08-13 01:04:15,304: Epoch 2 / 18, batch 1400 / 1605, 0.4456 sec/batch\n                         loss = [34.437077] gen = [0.401810] fr = [34.035267] spar = [0.000000] prec@1 = [31.250000] prec@5 = [62.500000] \n2024-08-13 01:04:59,863: Epoch 2 / 18, batch 1500 / 1605, 0.4456 sec/batch\n                         loss = [34.026009] gen = [0.403998] fr = [33.622009] spar = [0.000000] prec@1 = [37.500000] prec@5 = [62.500000] \n2024-08-13 01:05:44,415: Epoch 2 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [33.496632] gen = [0.329107] fr = [33.167526] spar = [0.000000] prec@1 = [56.250000] prec@5 = [62.500000] \n2024-08-13 01:05:47,654: Save checkpoint at epoch 2 ...\n2024-08-13 01:05:47,654: Current epoch 3, learning rate 0.01\n2024-08-13 01:05:48,252: Epoch 3 / 18, batch 1 / 1605, 0.5963 sec/batch\n                         loss = [35.880409] gen = [0.338911] fr = [35.541496] spar = [0.000000] prec@1 = [18.750000] prec@5 = [31.250000] \n2024-08-13 01:06:32,352: Epoch 3 / 18, batch 100 / 1605, 0.4470 sec/batch\n                         loss = [33.360210] gen = [0.315254] fr = [33.044956] spar = [0.000000] prec@1 = [37.500000] prec@5 = [43.750000] \n2024-08-13 01:07:16,910: Epoch 3 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [33.641350] gen = [0.413718] fr = [33.227631] spar = [0.000000] prec@1 = [56.250000] prec@5 = [75.000000] \n2024-08-13 01:08:01,469: Epoch 3 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [33.790436] gen = [0.358316] fr = [33.432121] spar = [0.000000] prec@1 = [37.500000] prec@5 = [50.000000] \n2024-08-13 01:08:46,031: Epoch 3 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [36.203396] gen = [0.312002] fr = [35.891396] spar = [0.000000] prec@1 = [18.750000] prec@5 = [43.750000] \n2024-08-13 01:09:30,572: Epoch 3 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [37.072704] gen = [0.305244] fr = [36.767460] spar = [0.000000] prec@1 = [25.000000] prec@5 = [50.000000] \n2024-08-13 01:10:15,133: Epoch 3 / 18, batch 600 / 1605, 0.4456 sec/batch\n                         loss = [33.146324] gen = [0.403405] fr = [32.742920] spar = [0.000000] prec@1 = [50.000000] prec@5 = [62.500000] \n2024-08-13 01:10:59,709: Epoch 3 / 18, batch 700 / 1605, 0.4457 sec/batch\n                         loss = [33.431297] gen = [0.371387] fr = [33.059910] spar = [0.000000] prec@1 = [50.000000] prec@5 = [75.000000] \n2024-08-13 01:11:44,250: Epoch 3 / 18, batch 800 / 1605, 0.4456 sec/batch\n                         loss = [33.948860] gen = [0.391831] fr = [33.557030] spar = [0.000000] prec@1 = [37.500000] prec@5 = [75.000000] \n2024-08-13 01:12:28,819: Epoch 3 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [33.305573] gen = [0.324522] fr = [32.981049] spar = [0.000000] prec@1 = [43.750000] prec@5 = [68.750000] \n2024-08-13 01:13:13,386: Epoch 3 / 18, batch 1000 / 1605, 0.4456 sec/batch\n                         loss = [33.333225] gen = [0.422521] fr = [32.910706] spar = [0.000000] prec@1 = [62.500000] prec@5 = [75.000000] \n2024-08-13 01:13:57,932: Epoch 3 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [33.004459] gen = [0.313209] fr = [32.691250] spar = [0.000000] prec@1 = [37.500000] prec@5 = [68.750000] \n2024-08-13 01:14:42,495: Epoch 3 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [32.062115] gen = [0.316793] fr = [31.745321] spar = [0.000000] prec@1 = [62.500000] prec@5 = [75.000000] \n2024-08-13 01:15:27,018: Epoch 3 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [30.963936] gen = [0.298491] fr = [30.665445] spar = [0.000000] prec@1 = [56.250000] prec@5 = [68.750000] \n2024-08-13 01:16:11,513: Epoch 3 / 18, batch 1400 / 1605, 0.4453 sec/batch\n                         loss = [33.162621] gen = [0.266601] fr = [32.896019] spar = [0.000000] prec@1 = [56.250000] prec@5 = [62.500000] \n2024-08-13 01:16:56,064: Epoch 3 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [32.688717] gen = [0.306853] fr = [32.381863] spar = [0.000000] prec@1 = [50.000000] prec@5 = [87.500000] \n2024-08-13 01:17:40,630: Epoch 3 / 18, batch 1600 / 1605, 0.4457 sec/batch\n                         loss = [32.903488] gen = [0.365815] fr = [32.537674] spar = [0.000000] prec@1 = [50.000000] prec@5 = [75.000000] \n2024-08-13 01:17:43,851: Save checkpoint at epoch 3 ...\n2024-08-13 01:17:43,851: Current epoch 4, learning rate 0.01\n2024-08-13 01:17:44,423: Epoch 4 / 18, batch 1 / 1605, 0.5709 sec/batch\n                         loss = [31.072855] gen = [0.384203] fr = [30.688652] spar = [0.000000] prec@1 = [62.500000] prec@5 = [87.500000] \n2024-08-13 01:18:28,542: Epoch 4 / 18, batch 100 / 1605, 0.4469 sec/batch\n                         loss = [32.370430] gen = [0.316743] fr = [32.053688] spar = [0.000000] prec@1 = [50.000000] prec@5 = [75.000000] \n2024-08-13 01:19:13,104: Epoch 4 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [30.525883] gen = [0.374400] fr = [30.151484] spar = [0.000000] prec@1 = [56.250000] prec@5 = [87.500000] \n2024-08-13 01:19:57,667: Epoch 4 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [29.305948] gen = [0.329078] fr = [28.976870] spar = [0.000000] prec@1 = [56.250000] prec@5 = [75.000000] \n2024-08-13 01:20:42,213: Epoch 4 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [30.501484] gen = [0.331726] fr = [30.169758] spar = [0.000000] prec@1 = [75.000000] prec@5 = [75.000000] \n2024-08-13 01:21:26,757: Epoch 4 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [28.926685] gen = [0.294534] fr = [28.632153] spar = [0.000000] prec@1 = [75.000000] prec@5 = [81.250000] \n2024-08-13 01:22:11,305: Epoch 4 / 18, batch 600 / 1605, 0.4455 sec/batch\n                         loss = [31.343882] gen = [0.272153] fr = [31.071730] spar = [0.000000] prec@1 = [50.000000] prec@5 = [62.500000] \n2024-08-13 01:22:55,858: Epoch 4 / 18, batch 700 / 1605, 0.4455 sec/batch\n                         loss = [33.007771] gen = [0.262408] fr = [32.745361] spar = [0.000000] prec@1 = [56.250000] prec@5 = [68.750000] \n2024-08-13 01:23:40,407: Epoch 4 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [30.448833] gen = [0.326596] fr = [30.122238] spar = [0.000000] prec@1 = [75.000000] prec@5 = [87.500000] \n2024-08-13 01:24:24,949: Epoch 4 / 18, batch 900 / 1605, 0.4455 sec/batch\n                         loss = [30.485710] gen = [0.374664] fr = [30.111046] spar = [0.000000] prec@1 = [62.500000] prec@5 = [81.250000] \n2024-08-13 01:25:09,499: Epoch 4 / 18, batch 1000 / 1605, 0.4455 sec/batch\n                         loss = [30.193954] gen = [0.294678] fr = [29.899277] spar = [0.000000] prec@1 = [68.750000] prec@5 = [81.250000] \n2024-08-13 01:25:54,031: Epoch 4 / 18, batch 1100 / 1605, 0.4453 sec/batch\n                         loss = [29.459438] gen = [0.272686] fr = [29.186752] spar = [0.000000] prec@1 = [56.250000] prec@5 = [81.250000] \n2024-08-13 01:26:38,553: Epoch 4 / 18, batch 1200 / 1605, 0.4453 sec/batch\n                         loss = [32.303223] gen = [0.278601] fr = [32.024624] spar = [0.000000] prec@1 = [43.750000] prec@5 = [68.750000] \n2024-08-13 01:27:23,083: Epoch 4 / 18, batch 1300 / 1605, 0.4453 sec/batch\n                         loss = [28.968662] gen = [0.273703] fr = [28.694960] spar = [0.000000] prec@1 = [75.000000] prec@5 = [87.500000] \n2024-08-13 01:28:07,647: Epoch 4 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [31.842642] gen = [0.298037] fr = [31.544605] spar = [0.000000] prec@1 = [56.250000] prec@5 = [87.500000] \n2024-08-13 01:28:52,200: Epoch 4 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [30.971590] gen = [0.361029] fr = [30.610561] spar = [0.000000] prec@1 = [81.250000] prec@5 = [81.250000] \n2024-08-13 01:29:36,753: Epoch 4 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [29.528189] gen = [0.258104] fr = [29.270084] spar = [0.000000] prec@1 = [75.000000] prec@5 = [93.750000] \n2024-08-13 01:29:40,015: Save checkpoint at epoch 4 ...\n2024-08-13 01:29:40,015: Current epoch 5, learning rate 0.01\n2024-08-13 01:29:40,606: Epoch 5 / 18, batch 1 / 1605, 0.5903 sec/batch\n                         loss = [26.819860] gen = [0.291773] fr = [26.528088] spar = [0.000000] prec@1 = [75.000000] prec@5 = [93.750000] \n2024-08-13 01:30:24,693: Epoch 5 / 18, batch 100 / 1605, 0.4468 sec/batch\n                         loss = [27.760033] gen = [0.367951] fr = [27.392082] spar = [0.000000] prec@1 = [68.750000] prec@5 = [87.500000] \n2024-08-13 01:31:09,257: Epoch 5 / 18, batch 200 / 1605, 0.4462 sec/batch\n                         loss = [28.448666] gen = [0.291730] fr = [28.156935] spar = [0.000000] prec@1 = [68.750000] prec@5 = [93.750000] \n2024-08-13 01:31:53,802: Epoch 5 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [32.944588] gen = [0.270479] fr = [32.674110] spar = [0.000000] prec@1 = [50.000000] prec@5 = [93.750000] \n2024-08-13 01:32:38,306: Epoch 5 / 18, batch 400 / 1605, 0.4457 sec/batch\n                         loss = [29.928926] gen = [0.251459] fr = [29.677467] spar = [0.000000] prec@1 = [81.250000] prec@5 = [87.500000] \n2024-08-13 01:33:22,877: Epoch 5 / 18, batch 500 / 1605, 0.4457 sec/batch\n                         loss = [29.807419] gen = [0.303575] fr = [29.503843] spar = [0.000000] prec@1 = [75.000000] prec@5 = [81.250000] \n2024-08-13 01:34:07,402: Epoch 5 / 18, batch 600 / 1605, 0.4453 sec/batch\n                         loss = [28.935022] gen = [0.294582] fr = [28.640440] spar = [0.000000] prec@1 = [62.500000] prec@5 = [93.750000] \n2024-08-13 01:34:51,929: Epoch 5 / 18, batch 700 / 1605, 0.4453 sec/batch\n                         loss = [29.830439] gen = [0.303168] fr = [29.527271] spar = [0.000000] prec@1 = [62.500000] prec@5 = [100.000000] \n2024-08-13 01:35:36,452: Epoch 5 / 18, batch 800 / 1605, 0.4453 sec/batch\n                         loss = [29.633369] gen = [0.238270] fr = [29.395100] spar = [0.000000] prec@1 = [68.750000] prec@5 = [93.750000] \n2024-08-13 01:36:21,010: Epoch 5 / 18, batch 900 / 1605, 0.4453 sec/batch\n                         loss = [30.904825] gen = [0.329702] fr = [30.575123] spar = [0.000000] prec@1 = [43.750000] prec@5 = [62.500000] \n2024-08-13 01:37:05,574: Epoch 5 / 18, batch 1000 / 1605, 0.4454 sec/batch\n                         loss = [29.676544] gen = [0.273827] fr = [29.402718] spar = [0.000000] prec@1 = [62.500000] prec@5 = [75.000000] \n2024-08-13 01:37:50,129: Epoch 5 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [28.352165] gen = [0.250462] fr = [28.101704] spar = [0.000000] prec@1 = [68.750000] prec@5 = [81.250000] \n2024-08-13 01:38:34,684: Epoch 5 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [30.824436] gen = [0.304471] fr = [30.519966] spar = [0.000000] prec@1 = [75.000000] prec@5 = [81.250000] \n2024-08-13 01:39:19,251: Epoch 5 / 18, batch 1300 / 1605, 0.4456 sec/batch\n                         loss = [28.574816] gen = [0.310088] fr = [28.264729] spar = [0.000000] prec@1 = [81.250000] prec@5 = [93.750000] \n2024-08-13 01:40:03,813: Epoch 5 / 18, batch 1400 / 1605, 0.4456 sec/batch\n                         loss = [28.200806] gen = [0.358290] fr = [27.842516] spar = [0.000000] prec@1 = [75.000000] prec@5 = [100.000000] \n2024-08-13 01:40:48,368: Epoch 5 / 18, batch 1500 / 1605, 0.4456 sec/batch\n                         loss = [28.569937] gen = [0.262606] fr = [28.307331] spar = [0.000000] prec@1 = [87.500000] prec@5 = [93.750000] \n2024-08-13 01:41:32,920: Epoch 5 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [27.695240] gen = [0.295829] fr = [27.399410] spar = [0.000000] prec@1 = [62.500000] prec@5 = [81.250000] \n2024-08-13 01:41:36,157: Save checkpoint at epoch 5 ...\n2024-08-13 01:41:36,157: Current epoch 6, learning rate 0.01\n2024-08-13 01:41:36,733: Epoch 6 / 18, batch 1 / 1605, 0.5745 sec/batch\n                         loss = [28.628883] gen = [0.249717] fr = [28.379166] spar = [0.000000] prec@1 = [68.750000] prec@5 = [93.750000] \n2024-08-13 01:42:20,815: Epoch 6 / 18, batch 100 / 1605, 0.4466 sec/batch\n                         loss = [26.943670] gen = [0.269697] fr = [26.673973] spar = [0.000000] prec@1 = [75.000000] prec@5 = [87.500000] \n2024-08-13 01:43:05,372: Epoch 6 / 18, batch 200 / 1605, 0.4461 sec/batch\n                         loss = [27.690453] gen = [0.350837] fr = [27.339615] spar = [0.000000] prec@1 = [75.000000] prec@5 = [93.750000] \n2024-08-13 01:43:49,944: Epoch 6 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [25.963917] gen = [0.266830] fr = [25.697086] spar = [0.000000] prec@1 = [62.500000] prec@5 = [100.000000] \n2024-08-13 01:44:34,499: Epoch 6 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [27.366180] gen = [0.261851] fr = [27.104328] spar = [0.000000] prec@1 = [81.250000] prec@5 = [87.500000] \n2024-08-13 01:45:19,053: Epoch 6 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [26.769749] gen = [0.276096] fr = [26.493652] spar = [0.000000] prec@1 = [81.250000] prec@5 = [93.750000] \n2024-08-13 01:46:03,618: Epoch 6 / 18, batch 600 / 1605, 0.4457 sec/batch\n                         loss = [27.817554] gen = [0.300780] fr = [27.516773] spar = [0.000000] prec@1 = [75.000000] prec@5 = [100.000000] \n2024-08-13 01:46:48,178: Epoch 6 / 18, batch 700 / 1605, 0.4456 sec/batch\n                         loss = [26.039648] gen = [0.330328] fr = [25.709320] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 01:47:32,737: Epoch 6 / 18, batch 800 / 1605, 0.4456 sec/batch\n                         loss = [24.773392] gen = [0.246529] fr = [24.526863] spar = [0.000000] prec@1 = [81.250000] prec@5 = [100.000000] \n2024-08-13 01:48:17,297: Epoch 6 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [25.674034] gen = [0.300966] fr = [25.373068] spar = [0.000000] prec@1 = [81.250000] prec@5 = [93.750000] \n2024-08-13 01:49:01,864: Epoch 6 / 18, batch 1000 / 1605, 0.4456 sec/batch\n                         loss = [28.573116] gen = [0.283034] fr = [28.290081] spar = [0.000000] prec@1 = [75.000000] prec@5 = [87.500000] \n2024-08-13 01:49:46,415: Epoch 6 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [26.363583] gen = [0.252841] fr = [26.110741] spar = [0.000000] prec@1 = [75.000000] prec@5 = [87.500000] \n2024-08-13 01:50:30,976: Epoch 6 / 18, batch 1200 / 1605, 0.4456 sec/batch\n                         loss = [23.825621] gen = [0.261414] fr = [23.564207] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 01:51:15,485: Epoch 6 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [22.692461] gen = [0.254782] fr = [22.437679] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 01:52:00,004: Epoch 6 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [25.080055] gen = [0.272237] fr = [24.807819] spar = [0.000000] prec@1 = [68.750000] prec@5 = [81.250000] \n2024-08-13 01:52:44,530: Epoch 6 / 18, batch 1500 / 1605, 0.4453 sec/batch\n                         loss = [25.240133] gen = [0.241715] fr = [24.998419] spar = [0.000000] prec@1 = [87.500000] prec@5 = [100.000000] \n2024-08-13 01:53:29,087: Epoch 6 / 18, batch 1600 / 1605, 0.4456 sec/batch\n                         loss = [28.288212] gen = [0.298844] fr = [27.989368] spar = [0.000000] prec@1 = [81.250000] prec@5 = [100.000000] \n2024-08-13 01:53:32,297: Save checkpoint at epoch 6 ...\n2024-08-13 01:53:32,297: Current epoch 7, learning rate 0.01\n2024-08-13 01:53:32,872: Epoch 7 / 18, batch 1 / 1605, 0.5733 sec/batch\n                         loss = [23.606913] gen = [0.264393] fr = [23.342520] spar = [0.000000] prec@1 = [93.750000] prec@5 = [93.750000] \n2024-08-13 01:54:16,924: Epoch 7 / 18, batch 100 / 1605, 0.4463 sec/batch\n                         loss = [24.509163] gen = [0.233062] fr = [24.276100] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 01:55:01,472: Epoch 7 / 18, batch 200 / 1605, 0.4459 sec/batch\n                         loss = [23.291359] gen = [0.251299] fr = [23.040060] spar = [0.000000] prec@1 = [87.500000] prec@5 = [87.500000] \n2024-08-13 01:55:45,946: Epoch 7 / 18, batch 300 / 1605, 0.4455 sec/batch\n                         loss = [22.358818] gen = [0.236482] fr = [22.122335] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 01:56:30,486: Epoch 7 / 18, batch 400 / 1605, 0.4455 sec/batch\n                         loss = [25.384310] gen = [0.238073] fr = [25.146236] spar = [0.000000] prec@1 = [81.250000] prec@5 = [93.750000] \n2024-08-13 01:57:15,023: Epoch 7 / 18, batch 500 / 1605, 0.4454 sec/batch\n                         loss = [22.112011] gen = [0.249928] fr = [21.862083] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 01:57:59,562: Epoch 7 / 18, batch 600 / 1605, 0.4454 sec/batch\n                         loss = [24.295647] gen = [0.225917] fr = [24.069731] spar = [0.000000] prec@1 = [87.500000] prec@5 = [100.000000] \n2024-08-13 01:58:44,119: Epoch 7 / 18, batch 700 / 1605, 0.4455 sec/batch\n                         loss = [22.455660] gen = [0.315858] fr = [22.139801] spar = [0.000000] prec@1 = [87.500000] prec@5 = [93.750000] \n2024-08-13 01:59:28,664: Epoch 7 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [22.677679] gen = [0.337018] fr = [22.340660] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:00:13,201: Epoch 7 / 18, batch 900 / 1605, 0.4454 sec/batch\n                         loss = [23.092018] gen = [0.313330] fr = [22.778688] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:00:57,763: Epoch 7 / 18, batch 1000 / 1605, 0.4455 sec/batch\n                         loss = [24.371330] gen = [0.235511] fr = [24.135818] spar = [0.000000] prec@1 = [93.750000] prec@5 = [93.750000] \n2024-08-13 02:01:42,338: Epoch 7 / 18, batch 1100 / 1605, 0.4457 sec/batch\n                         loss = [23.134106] gen = [0.286404] fr = [22.847702] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:02:26,897: Epoch 7 / 18, batch 1200 / 1605, 0.4457 sec/batch\n                         loss = [22.832790] gen = [0.362311] fr = [22.470480] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:03:11,459: Epoch 7 / 18, batch 1300 / 1605, 0.4457 sec/batch\n                         loss = [26.782650] gen = [0.271288] fr = [26.511362] spar = [0.000000] prec@1 = [68.750000] prec@5 = [87.500000] \n2024-08-13 02:03:56,029: Epoch 7 / 18, batch 1400 / 1605, 0.4457 sec/batch\n                         loss = [18.864834] gen = [0.211689] fr = [18.653145] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:04:40,588: Epoch 7 / 18, batch 1500 / 1605, 0.4456 sec/batch\n                         loss = [23.188339] gen = [0.225900] fr = [22.962439] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:05:25,147: Epoch 7 / 18, batch 1600 / 1605, 0.4456 sec/batch\n                         loss = [22.419197] gen = [0.228344] fr = [22.190853] spar = [0.000000] prec@1 = [81.250000] prec@5 = [93.750000] \n2024-08-13 02:05:28,386: Save checkpoint at epoch 7 ...\n2024-08-13 02:05:28,387: Current epoch 8, learning rate 0.01\n2024-08-13 02:05:28,957: Epoch 8 / 18, batch 1 / 1605, 0.5688 sec/batch\n                         loss = [18.859961] gen = [0.264877] fr = [18.595083] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:06:13,090: Epoch 8 / 18, batch 100 / 1605, 0.4470 sec/batch\n                         loss = [16.865852] gen = [0.195952] fr = [16.669901] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:06:57,645: Epoch 8 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [21.407820] gen = [0.224585] fr = [21.183235] spar = [0.000000] prec@1 = [81.250000] prec@5 = [100.000000] \n2024-08-13 02:07:42,205: Epoch 8 / 18, batch 300 / 1605, 0.4461 sec/batch\n                         loss = [20.360624] gen = [0.285371] fr = [20.075253] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:08:26,711: Epoch 8 / 18, batch 400 / 1605, 0.4458 sec/batch\n                         loss = [21.877005] gen = [0.270713] fr = [21.606293] spar = [0.000000] prec@1 = [87.500000] prec@5 = [100.000000] \n2024-08-13 02:09:11,245: Epoch 8 / 18, batch 500 / 1605, 0.4457 sec/batch\n                         loss = [19.027031] gen = [0.298632] fr = [18.728399] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:09:55,795: Epoch 8 / 18, batch 600 / 1605, 0.4455 sec/batch\n                         loss = [21.611599] gen = [0.289146] fr = [21.322453] spar = [0.000000] prec@1 = [93.750000] prec@5 = [93.750000] \n2024-08-13 02:10:40,364: Epoch 8 / 18, batch 700 / 1605, 0.4456 sec/batch\n                         loss = [21.352808] gen = [0.187662] fr = [21.165146] spar = [0.000000] prec@1 = [81.250000] prec@5 = [100.000000] \n2024-08-13 02:11:24,924: Epoch 8 / 18, batch 800 / 1605, 0.4456 sec/batch\n                         loss = [20.191721] gen = [0.220423] fr = [19.971298] spar = [0.000000] prec@1 = [87.500000] prec@5 = [100.000000] \n2024-08-13 02:12:09,488: Epoch 8 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [18.417896] gen = [0.234707] fr = [18.183189] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:12:54,069: Epoch 8 / 18, batch 1000 / 1605, 0.4456 sec/batch\n                         loss = [17.970362] gen = [0.234434] fr = [17.735928] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:13:38,623: Epoch 8 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [20.464649] gen = [0.247306] fr = [20.217344] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:14:23,193: Epoch 8 / 18, batch 1200 / 1605, 0.4456 sec/batch\n                         loss = [16.034058] gen = [0.277328] fr = [15.756729] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:15:07,701: Epoch 8 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [23.958807] gen = [0.285012] fr = [23.673796] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:15:52,254: Epoch 8 / 18, batch 1400 / 1605, 0.4455 sec/batch\n                         loss = [16.095308] gen = [0.345542] fr = [15.749767] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:16:36,798: Epoch 8 / 18, batch 1500 / 1605, 0.4455 sec/batch\n                         loss = [20.527521] gen = [0.304343] fr = [20.223179] spar = [0.000000] prec@1 = [87.500000] prec@5 = [93.750000] \n2024-08-13 02:17:21,369: Epoch 8 / 18, batch 1600 / 1605, 0.4457 sec/batch\n                         loss = [16.954929] gen = [0.224649] fr = [16.730280] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:17:24,613: Save checkpoint at epoch 8 ...\n2024-08-13 02:17:24,613: Current epoch 9, learning rate 0.01\n2024-08-13 02:17:25,216: Epoch 9 / 18, batch 1 / 1605, 0.6015 sec/batch\n                         loss = [17.153425] gen = [0.224099] fr = [16.929325] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:18:09,282: Epoch 9 / 18, batch 100 / 1605, 0.4467 sec/batch\n                         loss = [14.232011] gen = [0.301988] fr = [13.930022] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:18:53,849: Epoch 9 / 18, batch 200 / 1605, 0.4462 sec/batch\n                         loss = [17.597858] gen = [0.283242] fr = [17.314617] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:19:38,412: Epoch 9 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [15.715688] gen = [0.270990] fr = [15.444698] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:20:22,972: Epoch 9 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [17.835028] gen = [0.460532] fr = [17.374496] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:21:07,495: Epoch 9 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [16.270325] gen = [0.262727] fr = [16.007597] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:21:52,034: Epoch 9 / 18, batch 600 / 1605, 0.4454 sec/batch\n                         loss = [18.468081] gen = [0.272826] fr = [18.195255] spar = [0.000000] prec@1 = [93.750000] prec@5 = [93.750000] \n2024-08-13 02:22:36,530: Epoch 9 / 18, batch 700 / 1605, 0.4452 sec/batch\n                         loss = [14.688496] gen = [0.269875] fr = [14.418620] spar = [0.000000] prec@1 = [87.500000] prec@5 = [93.750000] \n2024-08-13 02:23:21,014: Epoch 9 / 18, batch 800 / 1605, 0.4451 sec/batch\n                         loss = [16.860361] gen = [0.232441] fr = [16.627920] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:24:05,507: Epoch 9 / 18, batch 900 / 1605, 0.4450 sec/batch\n                         loss = [18.382439] gen = [0.248566] fr = [18.133873] spar = [0.000000] prec@1 = [87.500000] prec@5 = [100.000000] \n2024-08-13 02:24:50,055: Epoch 9 / 18, batch 1000 / 1605, 0.4451 sec/batch\n                         loss = [17.055176] gen = [0.252679] fr = [16.802496] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:25:34,617: Epoch 9 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [13.708203] gen = [0.243057] fr = [13.465146] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:26:19,147: Epoch 9 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [15.977952] gen = [0.293943] fr = [15.684009] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:27:03,675: Epoch 9 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [16.117092] gen = [0.312455] fr = [15.804638] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:27:48,204: Epoch 9 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [19.581112] gen = [0.177860] fr = [19.403252] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:28:32,778: Epoch 9 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [17.595072] gen = [0.284452] fr = [17.310619] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:29:17,330: Epoch 9 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [12.555309] gen = [0.238158] fr = [12.317151] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:29:20,575: Save checkpoint at epoch 9 ...\n2024-08-13 02:29:20,575: Current epoch 10, learning rate 0.01\n2024-08-13 02:29:21,125: Epoch 10 / 18, batch 1 / 1605, 0.5483 sec/batch\n                         loss = [13.092261] gen = [0.277474] fr = [12.814788] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:30:05,249: Epoch 10 / 18, batch 100 / 1605, 0.4467 sec/batch\n                         loss = [12.158058] gen = [0.306532] fr = [11.851526] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:30:49,818: Epoch 10 / 18, batch 200 / 1605, 0.4462 sec/batch\n                         loss = [10.387243] gen = [0.298203] fr = [10.089040] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:31:34,361: Epoch 10 / 18, batch 300 / 1605, 0.4459 sec/batch\n                         loss = [15.231780] gen = [0.307021] fr = [14.924759] spar = [0.000000] prec@1 = [93.750000] prec@5 = [93.750000] \n2024-08-13 02:32:18,932: Epoch 10 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [12.739161] gen = [0.235505] fr = [12.503656] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:33:03,500: Epoch 10 / 18, batch 500 / 1605, 0.4458 sec/batch\n                         loss = [15.273873] gen = [0.238422] fr = [15.035452] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:33:48,047: Epoch 10 / 18, batch 600 / 1605, 0.4455 sec/batch\n                         loss = [12.605301] gen = [0.262600] fr = [12.342701] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:34:32,621: Epoch 10 / 18, batch 700 / 1605, 0.4456 sec/batch\n                         loss = [15.607511] gen = [0.286148] fr = [15.321362] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:35:17,153: Epoch 10 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [13.825552] gen = [0.221914] fr = [13.603639] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:36:01,675: Epoch 10 / 18, batch 900 / 1605, 0.4454 sec/batch\n                         loss = [9.022283] gen = [0.247040] fr = [8.775243] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:36:46,202: Epoch 10 / 18, batch 1000 / 1605, 0.4454 sec/batch\n                         loss = [10.003134] gen = [0.269136] fr = [9.733997] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:37:30,767: Epoch 10 / 18, batch 1100 / 1605, 0.4457 sec/batch\n                         loss = [12.065005] gen = [0.245563] fr = [11.819443] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:38:15,316: Epoch 10 / 18, batch 1200 / 1605, 0.4456 sec/batch\n                         loss = [10.678296] gen = [0.309763] fr = [10.368533] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:38:59,865: Epoch 10 / 18, batch 1300 / 1605, 0.4455 sec/batch\n                         loss = [12.732372] gen = [0.285462] fr = [12.446911] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:39:44,358: Epoch 10 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [8.031308] gen = [0.241440] fr = [7.789868] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:40:28,848: Epoch 10 / 18, batch 1500 / 1605, 0.4453 sec/batch\n                         loss = [11.347950] gen = [0.252794] fr = [11.095156] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:41:13,383: Epoch 10 / 18, batch 1600 / 1605, 0.4453 sec/batch\n                         loss = [12.725838] gen = [0.195511] fr = [12.530327] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:41:16,623: Save checkpoint at epoch 10 ...\n2024-08-13 02:41:16,624: Current epoch 11, learning rate 0.001\n2024-08-13 02:41:17,240: Epoch 11 / 18, batch 1 / 1605, 0.6151 sec/batch\n                         loss = [14.160319] gen = [0.269879] fr = [13.890440] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:42:01,348: Epoch 11 / 18, batch 100 / 1605, 0.4472 sec/batch\n                         loss = [7.061120] gen = [0.208243] fr = [6.852877] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:42:45,889: Epoch 11 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [7.130299] gen = [0.154451] fr = [6.975848] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:43:30,452: Epoch 11 / 18, batch 300 / 1605, 0.4461 sec/batch\n                         loss = [7.589038] gen = [0.150903] fr = [7.438135] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:44:14,875: Epoch 11 / 18, batch 400 / 1605, 0.4456 sec/batch\n                         loss = [5.572052] gen = [0.160847] fr = [5.411206] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:44:59,360: Epoch 11 / 18, batch 500 / 1605, 0.4455 sec/batch\n                         loss = [6.677022] gen = [0.173842] fr = [6.503181] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:45:43,868: Epoch 11 / 18, batch 600 / 1605, 0.4451 sec/batch\n                         loss = [4.485564] gen = [0.151510] fr = [4.334054] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:46:28,428: Epoch 11 / 18, batch 700 / 1605, 0.4453 sec/batch\n                         loss = [10.000637] gen = [0.145308] fr = [9.855329] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:47:12,963: Epoch 11 / 18, batch 800 / 1605, 0.4453 sec/batch\n                         loss = [7.490114] gen = [0.146514] fr = [7.343601] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:47:57,538: Epoch 11 / 18, batch 900 / 1605, 0.4454 sec/batch\n                         loss = [2.455419] gen = [0.158784] fr = [2.296635] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:48:42,108: Epoch 11 / 18, batch 1000 / 1605, 0.4455 sec/batch\n                         loss = [5.728913] gen = [0.145273] fr = [5.583640] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:49:26,671: Epoch 11 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [9.308150] gen = [0.152072] fr = [9.156078] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:50:11,176: Epoch 11 / 18, batch 1200 / 1605, 0.4453 sec/batch\n                         loss = [6.354498] gen = [0.133851] fr = [6.220646] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:50:55,730: Epoch 11 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [6.189884] gen = [0.178106] fr = [6.011778] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:51:40,265: Epoch 11 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [4.744094] gen = [0.142996] fr = [4.601099] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:52:24,814: Epoch 11 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [3.908245] gen = [0.147584] fr = [3.760661] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:53:09,359: Epoch 11 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [3.346955] gen = [0.147650] fr = [3.199305] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:53:12,591: Save checkpoint at epoch 11 ...\n2024-08-13 02:53:12,591: Current epoch 12, learning rate 0.001\n2024-08-13 02:53:13,210: Epoch 12 / 18, batch 1 / 1605, 0.6170 sec/batch\n                         loss = [3.947132] gen = [0.132901] fr = [3.814230] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:53:57,305: Epoch 12 / 18, batch 100 / 1605, 0.4471 sec/batch\n                         loss = [5.055677] gen = [0.175430] fr = [4.880247] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:54:41,878: Epoch 12 / 18, batch 200 / 1605, 0.4464 sec/batch\n                         loss = [3.156923] gen = [0.138662] fr = [3.018261] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:55:26,431: Epoch 12 / 18, batch 300 / 1605, 0.4461 sec/batch\n                         loss = [4.229269] gen = [0.156932] fr = [4.072336] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:56:10,997: Epoch 12 / 18, batch 400 / 1605, 0.4460 sec/batch\n                         loss = [3.387597] gen = [0.132308] fr = [3.255289] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:56:55,568: Epoch 12 / 18, batch 500 / 1605, 0.4459 sec/batch\n                         loss = [6.348648] gen = [0.179197] fr = [6.169451] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 02:57:40,113: Epoch 12 / 18, batch 600 / 1605, 0.4455 sec/batch\n                         loss = [4.170289] gen = [0.167779] fr = [4.002510] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:58:24,667: Epoch 12 / 18, batch 700 / 1605, 0.4455 sec/batch\n                         loss = [5.310305] gen = [0.155438] fr = [5.154867] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:59:09,213: Epoch 12 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [3.408947] gen = [0.135047] fr = [3.273900] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 02:59:53,780: Epoch 12 / 18, batch 900 / 1605, 0.4455 sec/batch\n                         loss = [3.515812] gen = [0.151041] fr = [3.364772] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:00:38,341: Epoch 12 / 18, batch 1000 / 1605, 0.4455 sec/batch\n                         loss = [2.976998] gen = [0.146465] fr = [2.830533] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:01:22,898: Epoch 12 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [4.068471] gen = [0.172861] fr = [3.895610] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:02:07,412: Epoch 12 / 18, batch 1200 / 1605, 0.4454 sec/batch\n                         loss = [3.355709] gen = [0.174413] fr = [3.181296] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:02:51,919: Epoch 12 / 18, batch 1300 / 1605, 0.4453 sec/batch\n                         loss = [3.962189] gen = [0.134772] fr = [3.827417] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:03:36,472: Epoch 12 / 18, batch 1400 / 1605, 0.4453 sec/batch\n                         loss = [4.968611] gen = [0.154581] fr = [4.814031] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:04:21,041: Epoch 12 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [6.484193] gen = [0.161909] fr = [6.322284] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:05:05,591: Epoch 12 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [3.370924] gen = [0.146318] fr = [3.224605] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:05:08,806: Save checkpoint at epoch 12 ...\n2024-08-13 03:05:08,806: Current epoch 13, learning rate 0.001\n2024-08-13 03:05:09,396: Epoch 13 / 18, batch 1 / 1605, 0.5888 sec/batch\n                         loss = [3.101963] gen = [0.125156] fr = [2.976806] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:05:53,509: Epoch 13 / 18, batch 100 / 1605, 0.4470 sec/batch\n                         loss = [2.518604] gen = [0.161911] fr = [2.356693] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:06:38,073: Epoch 13 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [2.333067] gen = [0.163288] fr = [2.169779] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:07:22,636: Epoch 13 / 18, batch 300 / 1605, 0.4461 sec/batch\n                         loss = [2.564994] gen = [0.160472] fr = [2.404521] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:08:07,207: Epoch 13 / 18, batch 400 / 1605, 0.4460 sec/batch\n                         loss = [2.878005] gen = [0.139008] fr = [2.738997] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:08:51,760: Epoch 13 / 18, batch 500 / 1605, 0.4459 sec/batch\n                         loss = [4.024435] gen = [0.146564] fr = [3.877870] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:09:36,304: Epoch 13 / 18, batch 600 / 1605, 0.4454 sec/batch\n                         loss = [2.454241] gen = [0.135097] fr = [2.319144] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:10:20,870: Epoch 13 / 18, batch 700 / 1605, 0.4455 sec/batch\n                         loss = [3.994804] gen = [0.145211] fr = [3.849593] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:11:05,407: Epoch 13 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [5.099225] gen = [0.139731] fr = [4.959494] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:11:49,937: Epoch 13 / 18, batch 900 / 1605, 0.4454 sec/batch\n                         loss = [2.875241] gen = [0.134795] fr = [2.740446] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:12:34,484: Epoch 13 / 18, batch 1000 / 1605, 0.4454 sec/batch\n                         loss = [1.785471] gen = [0.132246] fr = [1.653226] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:13:19,041: Epoch 13 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [3.383659] gen = [0.133536] fr = [3.250123] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:14:03,580: Epoch 13 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [2.957748] gen = [0.138729] fr = [2.819019] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:14:48,133: Epoch 13 / 18, batch 1300 / 1605, 0.4455 sec/batch\n                         loss = [4.742290] gen = [0.137161] fr = [4.605130] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:15:32,675: Epoch 13 / 18, batch 1400 / 1605, 0.4455 sec/batch\n                         loss = [3.459476] gen = [0.135907] fr = [3.323569] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:16:17,243: Epoch 13 / 18, batch 1500 / 1605, 0.4455 sec/batch\n                         loss = [4.749304] gen = [0.161858] fr = [4.587446] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:17:01,793: Epoch 13 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [2.106611] gen = [0.143789] fr = [1.962823] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:17:04,997: Save checkpoint at epoch 13 ...\n2024-08-13 03:17:04,997: Current epoch 14, learning rate 0.001\n2024-08-13 03:17:05,596: Epoch 14 / 18, batch 1 / 1605, 0.5977 sec/batch\n                         loss = [3.076012] gen = [0.136114] fr = [2.939898] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:17:49,683: Epoch 14 / 18, batch 100 / 1605, 0.4468 sec/batch\n                         loss = [2.743660] gen = [0.144718] fr = [2.598942] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:18:34,245: Epoch 14 / 18, batch 200 / 1605, 0.4462 sec/batch\n                         loss = [2.862851] gen = [0.125145] fr = [2.737707] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:19:18,803: Epoch 14 / 18, batch 300 / 1605, 0.4460 sec/batch\n                         loss = [1.559821] gen = [0.135637] fr = [1.424184] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:20:03,357: Epoch 14 / 18, batch 400 / 1605, 0.4459 sec/batch\n                         loss = [1.867179] gen = [0.131498] fr = [1.735682] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:20:47,925: Epoch 14 / 18, batch 500 / 1605, 0.4459 sec/batch\n                         loss = [6.940181] gen = [0.123215] fr = [6.816967] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:21:32,396: Epoch 14 / 18, batch 600 / 1605, 0.4447 sec/batch\n                         loss = [2.919780] gen = [0.122956] fr = [2.796824] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:22:16,807: Epoch 14 / 18, batch 700 / 1605, 0.4444 sec/batch\n                         loss = [3.020538] gen = [0.120836] fr = [2.899702] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:23:01,248: Epoch 14 / 18, batch 800 / 1605, 0.4444 sec/batch\n                         loss = [2.571328] gen = [0.159699] fr = [2.411629] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:23:45,794: Epoch 14 / 18, batch 900 / 1605, 0.4447 sec/batch\n                         loss = [3.226646] gen = [0.135030] fr = [3.091616] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:24:30,349: Epoch 14 / 18, batch 1000 / 1605, 0.4448 sec/batch\n                         loss = [1.109473] gen = [0.149817] fr = [0.959656] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:25:14,928: Epoch 14 / 18, batch 1100 / 1605, 0.4458 sec/batch\n                         loss = [0.551661] gen = [0.140497] fr = [0.411164] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:25:59,486: Epoch 14 / 18, batch 1200 / 1605, 0.4457 sec/batch\n                         loss = [2.642712] gen = [0.155190] fr = [2.487522] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:26:44,041: Epoch 14 / 18, batch 1300 / 1605, 0.4456 sec/batch\n                         loss = [1.640478] gen = [0.128438] fr = [1.512041] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:27:28,608: Epoch 14 / 18, batch 1400 / 1605, 0.4456 sec/batch\n                         loss = [4.439907] gen = [0.127096] fr = [4.312810] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:28:13,182: Epoch 14 / 18, batch 1500 / 1605, 0.4457 sec/batch\n                         loss = [2.342613] gen = [0.136334] fr = [2.206279] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:28:57,736: Epoch 14 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [2.154317] gen = [0.151778] fr = [2.002539] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:29:00,947: Save checkpoint at epoch 14 ...\n2024-08-13 03:29:00,948: Current epoch 15, learning rate 0.001\n2024-08-13 03:29:01,508: Epoch 15 / 18, batch 1 / 1605, 0.5590 sec/batch\n                         loss = [1.558606] gen = [0.142926] fr = [1.415680] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:29:45,571: Epoch 15 / 18, batch 100 / 1605, 0.4462 sec/batch\n                         loss = [1.752858] gen = [0.137333] fr = [1.615525] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:30:30,140: Epoch 15 / 18, batch 200 / 1605, 0.4460 sec/batch\n                         loss = [3.702000] gen = [0.137581] fr = [3.564418] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:31:14,695: Epoch 15 / 18, batch 300 / 1605, 0.4458 sec/batch\n                         loss = [1.355086] gen = [0.146932] fr = [1.208155] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:31:59,263: Epoch 15 / 18, batch 400 / 1605, 0.4458 sec/batch\n                         loss = [3.367796] gen = [0.140767] fr = [3.227030] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:32:43,823: Epoch 15 / 18, batch 500 / 1605, 0.4457 sec/batch\n                         loss = [1.014699] gen = [0.136498] fr = [0.878201] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:33:28,351: Epoch 15 / 18, batch 600 / 1605, 0.4453 sec/batch\n                         loss = [1.638267] gen = [0.127516] fr = [1.510750] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:34:12,901: Epoch 15 / 18, batch 700 / 1605, 0.4454 sec/batch\n                         loss = [2.991535] gen = [0.137370] fr = [2.854165] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:34:57,395: Epoch 15 / 18, batch 800 / 1605, 0.4452 sec/batch\n                         loss = [0.773495] gen = [0.125925] fr = [0.647570] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:35:41,909: Epoch 15 / 18, batch 900 / 1605, 0.4452 sec/batch\n                         loss = [2.128477] gen = [0.139475] fr = [1.989003] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:36:26,457: Epoch 15 / 18, batch 1000 / 1605, 0.4453 sec/batch\n                         loss = [1.296462] gen = [0.140603] fr = [1.155859] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:37:11,006: Epoch 15 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [2.350433] gen = [0.135850] fr = [2.214584] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:37:55,522: Epoch 15 / 18, batch 1200 / 1605, 0.4453 sec/batch\n                         loss = [2.112361] gen = [0.141282] fr = [1.971079] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:38:40,028: Epoch 15 / 18, batch 1300 / 1605, 0.4452 sec/batch\n                         loss = [3.208049] gen = [0.127355] fr = [3.080694] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:39:24,564: Epoch 15 / 18, batch 1400 / 1605, 0.4453 sec/batch\n                         loss = [1.984160] gen = [0.135257] fr = [1.848903] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:40:09,112: Epoch 15 / 18, batch 1500 / 1605, 0.4453 sec/batch\n                         loss = [3.255420] gen = [0.141704] fr = [3.113716] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:40:53,685: Epoch 15 / 18, batch 1600 / 1605, 0.4457 sec/batch\n                         loss = [2.781090] gen = [0.126723] fr = [2.654368] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:40:56,909: Save checkpoint at epoch 15 ...\n2024-08-13 03:40:56,909: Current epoch 16, learning rate 0.001\n2024-08-13 03:40:57,518: Epoch 16 / 18, batch 1 / 1605, 0.6075 sec/batch\n                         loss = [2.663132] gen = [0.137469] fr = [2.525663] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:41:41,642: Epoch 16 / 18, batch 100 / 1605, 0.4473 sec/batch\n                         loss = [1.066417] gen = [0.122516] fr = [0.943900] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:42:26,176: Epoch 16 / 18, batch 200 / 1605, 0.4463 sec/batch\n                         loss = [1.507649] gen = [0.111016] fr = [1.396634] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:43:10,744: Epoch 16 / 18, batch 300 / 1605, 0.4461 sec/batch\n                         loss = [2.267857] gen = [0.133918] fr = [2.133939] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:43:55,304: Epoch 16 / 18, batch 400 / 1605, 0.4460 sec/batch\n                         loss = [1.705605] gen = [0.135049] fr = [1.570556] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:44:39,855: Epoch 16 / 18, batch 500 / 1605, 0.4459 sec/batch\n                         loss = [2.279568] gen = [0.122823] fr = [2.156744] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:45:24,313: Epoch 16 / 18, batch 600 / 1605, 0.4446 sec/batch\n                         loss = [3.762078] gen = [0.153786] fr = [3.608293] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:46:08,690: Epoch 16 / 18, batch 700 / 1605, 0.4442 sec/batch\n                         loss = [0.344677] gen = [0.142132] fr = [0.202545] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:46:53,243: Epoch 16 / 18, batch 800 / 1605, 0.4446 sec/batch\n                         loss = [0.591929] gen = [0.137475] fr = [0.454454] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:47:37,812: Epoch 16 / 18, batch 900 / 1605, 0.4449 sec/batch\n                         loss = [0.774306] gen = [0.124236] fr = [0.650070] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:48:22,376: Epoch 16 / 18, batch 1000 / 1605, 0.4450 sec/batch\n                         loss = [2.091932] gen = [0.119731] fr = [1.972201] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:49:06,939: Epoch 16 / 18, batch 1100 / 1605, 0.4456 sec/batch\n                         loss = [2.291449] gen = [0.119688] fr = [2.171762] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:49:51,478: Epoch 16 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [1.862771] gen = [0.116572] fr = [1.746199] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:50:36,002: Epoch 16 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [2.360733] gen = [0.130285] fr = [2.230449] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:51:20,526: Epoch 16 / 18, batch 1400 / 1605, 0.4454 sec/batch\n                         loss = [2.734187] gen = [0.109337] fr = [2.624850] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:52:05,076: Epoch 16 / 18, batch 1500 / 1605, 0.4454 sec/batch\n                         loss = [3.568964] gen = [0.150101] fr = [3.418864] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:52:49,639: Epoch 16 / 18, batch 1600 / 1605, 0.4456 sec/batch\n                         loss = [1.022506] gen = [0.141328] fr = [0.881179] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:52:52,825: Save checkpoint at epoch 16 ...\n2024-08-13 03:52:52,825: Current epoch 17, learning rate 0.001\n2024-08-13 03:52:53,410: Epoch 17 / 18, batch 1 / 1605, 0.5837 sec/batch\n                         loss = [3.137331] gen = [0.127059] fr = [3.010272] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:53:37,485: Epoch 17 / 18, batch 100 / 1605, 0.4466 sec/batch\n                         loss = [0.581006] gen = [0.120594] fr = [0.460412] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:54:21,971: Epoch 17 / 18, batch 200 / 1605, 0.4457 sec/batch\n                         loss = [2.179053] gen = [0.155690] fr = [2.023363] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:55:06,456: Epoch 17 / 18, batch 300 / 1605, 0.4454 sec/batch\n                         loss = [3.794920] gen = [0.123725] fr = [3.671195] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 03:55:50,989: Epoch 17 / 18, batch 400 / 1605, 0.4454 sec/batch\n                         loss = [1.252633] gen = [0.127576] fr = [1.125057] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:56:35,542: Epoch 17 / 18, batch 500 / 1605, 0.4454 sec/batch\n                         loss = [0.787063] gen = [0.130434] fr = [0.656630] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:57:20,103: Epoch 17 / 18, batch 600 / 1605, 0.4456 sec/batch\n                         loss = [0.907553] gen = [0.150630] fr = [0.756923] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:58:04,660: Epoch 17 / 18, batch 700 / 1605, 0.4456 sec/batch\n                         loss = [0.466416] gen = [0.132500] fr = [0.333915] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:58:49,206: Epoch 17 / 18, batch 800 / 1605, 0.4455 sec/batch\n                         loss = [0.436096] gen = [0.135226] fr = [0.300870] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 03:59:33,771: Epoch 17 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [0.871520] gen = [0.141514] fr = [0.730006] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:00:18,339: Epoch 17 / 18, batch 1000 / 1605, 0.4456 sec/batch\n                         loss = [3.696377] gen = [0.122163] fr = [3.574214] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 04:01:02,887: Epoch 17 / 18, batch 1100 / 1605, 0.4455 sec/batch\n                         loss = [3.211817] gen = [0.137895] fr = [3.073923] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 04:01:47,429: Epoch 17 / 18, batch 1200 / 1605, 0.4455 sec/batch\n                         loss = [1.261045] gen = [0.120045] fr = [1.141000] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:02:31,971: Epoch 17 / 18, batch 1300 / 1605, 0.4454 sec/batch\n                         loss = [5.000597] gen = [0.135530] fr = [4.865067] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 04:03:16,529: Epoch 17 / 18, batch 1400 / 1605, 0.4455 sec/batch\n                         loss = [1.552477] gen = [0.105137] fr = [1.447341] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:04:01,085: Epoch 17 / 18, batch 1500 / 1605, 0.4455 sec/batch\n                         loss = [0.586658] gen = [0.127535] fr = [0.459123] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:04:45,633: Epoch 17 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [2.432384] gen = [0.123872] fr = [2.308512] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:04:48,849: Save checkpoint at epoch 17 ...\n2024-08-13 04:04:48,849: Current epoch 18, learning rate 0.001\n2024-08-13 04:04:49,416: Epoch 18 / 18, batch 1 / 1605, 0.5660 sec/batch\n                         loss = [4.843524] gen = [0.122741] fr = [4.720783] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 04:05:33,531: Epoch 18 / 18, batch 100 / 1605, 0.4468 sec/batch\n                         loss = [0.416967] gen = [0.114691] fr = [0.302276] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:06:18,087: Epoch 18 / 18, batch 200 / 1605, 0.4462 sec/batch\n                         loss = [0.451990] gen = [0.132799] fr = [0.319192] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:07:02,583: Epoch 18 / 18, batch 300 / 1605, 0.4458 sec/batch\n                         loss = [1.072193] gen = [0.118898] fr = [0.953295] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:07:47,002: Epoch 18 / 18, batch 400 / 1605, 0.4454 sec/batch\n                         loss = [4.961777] gen = [0.126387] fr = [4.835391] spar = [0.000000] prec@1 = [93.750000] prec@5 = [100.000000] \n2024-08-13 04:08:31,561: Epoch 18 / 18, batch 500 / 1605, 0.4454 sec/batch\n                         loss = [0.987619] gen = [0.123459] fr = [0.864160] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:09:16,133: Epoch 18 / 18, batch 600 / 1605, 0.4457 sec/batch\n                         loss = [0.540999] gen = [0.120888] fr = [0.420111] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:10:00,700: Epoch 18 / 18, batch 700 / 1605, 0.4457 sec/batch\n                         loss = [1.276637] gen = [0.143647] fr = [1.132990] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:10:45,252: Epoch 18 / 18, batch 800 / 1605, 0.4456 sec/batch\n                         loss = [0.637990] gen = [0.115761] fr = [0.522229] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:11:29,804: Epoch 18 / 18, batch 900 / 1605, 0.4456 sec/batch\n                         loss = [0.478323] gen = [0.113574] fr = [0.364750] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:12:14,209: Epoch 18 / 18, batch 1000 / 1605, 0.4453 sec/batch\n                         loss = [0.395046] gen = [0.112472] fr = [0.282573] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:12:58,622: Epoch 18 / 18, batch 1100 / 1605, 0.4441 sec/batch\n                         loss = [2.343390] gen = [0.117660] fr = [2.225730] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:13:43,140: Epoch 18 / 18, batch 1200 / 1605, 0.4447 sec/batch\n                         loss = [0.513408] gen = [0.126277] fr = [0.387131] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:14:27,715: Epoch 18 / 18, batch 1300 / 1605, 0.4450 sec/batch\n                         loss = [0.430469] gen = [0.128529] fr = [0.301939] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:15:12,261: Epoch 18 / 18, batch 1400 / 1605, 0.4451 sec/batch\n                         loss = [0.784665] gen = [0.121144] fr = [0.663521] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:15:56,823: Epoch 18 / 18, batch 1500 / 1605, 0.4452 sec/batch\n                         loss = [0.274840] gen = [0.129730] fr = [0.145110] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:16:41,372: Epoch 18 / 18, batch 1600 / 1605, 0.4455 sec/batch\n                         loss = [1.229092] gen = [0.120194] fr = [1.108898] spar = [0.000000] prec@1 = [100.000000] prec@5 = [100.000000] \n2024-08-13 04:16:44,589: Save checkpoint at epoch 18 ...\n","output_type":"stream"}]}]}